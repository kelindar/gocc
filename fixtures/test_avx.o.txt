
test_avx.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 <uint8_mul>:
   0:	55                                              	push   %rbp
   1:	48 89 e5                                        	mov    %rsp,%rbp
   4:	48 83 e4 f8                                     	and    $0xfffffffffffffff8,%rsp
   8:	85 c9                                           	test   %ecx,%ecx
   a:	0f 8e 96 00 00 00                               	jle    a6 <uint8_mul+0xa6>
  10:	41 89 c8                                        	mov    %ecx,%r8d
  13:	49 83 f8 10                                     	cmp    $0x10,%r8
  17:	0f 83 91 00 00 00                               	jae    ae <uint8_mul+0xae>
  1d:	45 31 c9                                        	xor    %r9d,%r9d
  20:	44 29 c9                                        	sub    %r9d,%ecx
  23:	4d 89 ca                                        	mov    %r9,%r10
  26:	49 f7 d2                                        	not    %r10
  29:	4d 01 c2                                        	add    %r8,%r10
  2c:	48 83 e1 03                                     	and    $0x3,%rcx
  30:	74 23                                           	je     55 <uint8_mul+0x55>
  32:	66 2e 0f 1f 84 00 00 00 00 00                   	cs nopw 0x0(%rax,%rax,1)
  3c:	0f 1f 40 00                                     	nopl   0x0(%rax)
  40:	42 0f b6 04 0e                                  	movzbl (%rsi,%r9,1),%eax
  45:	42 f6 24 0f                                     	mulb   (%rdi,%r9,1)
  49:	42 88 04 0a                                     	mov    %al,(%rdx,%r9,1)
  4d:	49 ff c1                                        	inc    %r9
  50:	48 ff c9                                        	dec    %rcx
  53:	75 eb                                           	jne    40 <uint8_mul+0x40>
  55:	49 83 fa 03                                     	cmp    $0x3,%r10
  59:	72 4b                                           	jb     a6 <uint8_mul+0xa6>
  5b:	0f 1f 44 00 00                                  	nopl   0x0(%rax,%rax,1)
  60:	42 0f b6 04 0e                                  	movzbl (%rsi,%r9,1),%eax
  65:	42 f6 24 0f                                     	mulb   (%rdi,%r9,1)
  69:	42 88 04 0a                                     	mov    %al,(%rdx,%r9,1)
  6d:	42 0f b6 44 0e 01                               	movzbl 0x1(%rsi,%r9,1),%eax
  73:	42 f6 64 0f 01                                  	mulb   0x1(%rdi,%r9,1)
  78:	42 88 44 0a 01                                  	mov    %al,0x1(%rdx,%r9,1)
  7d:	42 0f b6 44 0e 02                               	movzbl 0x2(%rsi,%r9,1),%eax
  83:	42 f6 64 0f 02                                  	mulb   0x2(%rdi,%r9,1)
  88:	42 88 44 0a 02                                  	mov    %al,0x2(%rdx,%r9,1)
  8d:	42 0f b6 44 0e 03                               	movzbl 0x3(%rsi,%r9,1),%eax
  93:	42 f6 64 0f 03                                  	mulb   0x3(%rdi,%r9,1)
  98:	42 88 44 0a 03                                  	mov    %al,0x3(%rdx,%r9,1)
  9d:	49 83 c1 04                                     	add    $0x4,%r9
  a1:	4d 39 c8                                        	cmp    %r9,%r8
  a4:	75 ba                                           	jne    60 <uint8_mul+0x60>
  a6:	48 89 ec                                        	mov    %rbp,%rsp
  a9:	5d                                              	pop    %rbp
  aa:	c5 f8 77                                        	vzeroupper 
  ad:	c3                                              	ret    
  ae:	48 89 d0                                        	mov    %rdx,%rax
  b1:	48 29 f8                                        	sub    %rdi,%rax
  b4:	45 31 c9                                        	xor    %r9d,%r9d
  b7:	48 3d 80 00 00 00                               	cmp    $0x80,%rax
  bd:	0f 82 5d ff ff ff                               	jb     20 <uint8_mul+0x20>
  c3:	48 89 d0                                        	mov    %rdx,%rax
  c6:	48 29 f0                                        	sub    %rsi,%rax
  c9:	48 3d 80 00 00 00                               	cmp    $0x80,%rax
  cf:	0f 82 4b ff ff ff                               	jb     20 <uint8_mul+0x20>
  d5:	41 81 f8 80 00 00 00                            	cmp    $0x80,%r8d
  dc:	73 08                                           	jae    e6 <uint8_mul+0xe6>
  de:	45 31 c9                                        	xor    %r9d,%r9d
  e1:	e9 13 01 00 00                                  	jmp    1f9 <uint8_mul+0x1f9>
  e6:	41 89 ca                                        	mov    %ecx,%r10d
  e9:	41 83 e2 7f                                     	and    $0x7f,%r10d
  ed:	4d 89 c1                                        	mov    %r8,%r9
  f0:	4d 29 d1                                        	sub    %r10,%r9
  f3:	31 c0                                           	xor    %eax,%eax
  f5:	c5 fd 6f 05 00 00 00 00                         	vmovdqa 0x0(%rip),%ymm0        # fd <uint8_mul+0xfd>
  fd:	0f 1f 00                                        	nopl   (%rax)
 100:	c5 fe 6f 1c 07                                  	vmovdqu (%rdi,%rax,1),%ymm3
 105:	c5 fe 6f 64 07 20                               	vmovdqu 0x20(%rdi,%rax,1),%ymm4
 10b:	c5 fe 6f 6c 07 40                               	vmovdqu 0x40(%rdi,%rax,1),%ymm5
 111:	c5 fe 6f 4c 07 60                               	vmovdqu 0x60(%rdi,%rax,1),%ymm1
 117:	c5 fe 6f 34 06                                  	vmovdqu (%rsi,%rax,1),%ymm6
 11c:	c5 fe 6f 7c 06 20                               	vmovdqu 0x20(%rsi,%rax,1),%ymm7
 122:	c5 7e 6f 44 06 40                               	vmovdqu 0x40(%rsi,%rax,1),%ymm8
 128:	c5 fe 6f 54 06 60                               	vmovdqu 0x60(%rsi,%rax,1),%ymm2
 12e:	c5 65 68 cb                                     	vpunpckhbw %ymm3,%ymm3,%ymm9
 132:	c5 4d 68 d6                                     	vpunpckhbw %ymm6,%ymm6,%ymm10
 136:	c4 41 2d d5 c9                                  	vpmullw %ymm9,%ymm10,%ymm9
 13b:	c5 35 db c8                                     	vpand  %ymm0,%ymm9,%ymm9
 13f:	c5 e5 60 db                                     	vpunpcklbw %ymm3,%ymm3,%ymm3
 143:	c5 cd 60 f6                                     	vpunpcklbw %ymm6,%ymm6,%ymm6
 147:	c5 cd d5 db                                     	vpmullw %ymm3,%ymm6,%ymm3
 14b:	c5 e5 db d8                                     	vpand  %ymm0,%ymm3,%ymm3
 14f:	c4 c1 65 67 d9                                  	vpackuswb %ymm9,%ymm3,%ymm3
 154:	c5 dd 68 f4                                     	vpunpckhbw %ymm4,%ymm4,%ymm6
 158:	c5 45 68 cf                                     	vpunpckhbw %ymm7,%ymm7,%ymm9
 15c:	c5 b5 d5 f6                                     	vpmullw %ymm6,%ymm9,%ymm6
 160:	c5 cd db f0                                     	vpand  %ymm0,%ymm6,%ymm6
 164:	c5 dd 60 e4                                     	vpunpcklbw %ymm4,%ymm4,%ymm4
 168:	c5 c5 60 ff                                     	vpunpcklbw %ymm7,%ymm7,%ymm7
 16c:	c5 c5 d5 e4                                     	vpmullw %ymm4,%ymm7,%ymm4
 170:	c5 dd db e0                                     	vpand  %ymm0,%ymm4,%ymm4
 174:	c5 dd 67 e6                                     	vpackuswb %ymm6,%ymm4,%ymm4
 178:	c5 d5 68 f5                                     	vpunpckhbw %ymm5,%ymm5,%ymm6
 17c:	c4 c1 3d 68 f8                                  	vpunpckhbw %ymm8,%ymm8,%ymm7
 181:	c5 c5 d5 f6                                     	vpmullw %ymm6,%ymm7,%ymm6
 185:	c5 cd db f0                                     	vpand  %ymm0,%ymm6,%ymm6
 189:	c5 d5 60 ed                                     	vpunpcklbw %ymm5,%ymm5,%ymm5
 18d:	c4 c1 3d 60 f8                                  	vpunpcklbw %ymm8,%ymm8,%ymm7
 192:	c5 c5 d5 ed                                     	vpmullw %ymm5,%ymm7,%ymm5
 196:	c5 d5 db e8                                     	vpand  %ymm0,%ymm5,%ymm5
 19a:	c5 d5 67 ee                                     	vpackuswb %ymm6,%ymm5,%ymm5
 19e:	c5 f5 68 f1                                     	vpunpckhbw %ymm1,%ymm1,%ymm6
 1a2:	c5 ed 68 fa                                     	vpunpckhbw %ymm2,%ymm2,%ymm7
 1a6:	c5 c5 d5 f6                                     	vpmullw %ymm6,%ymm7,%ymm6
 1aa:	c5 cd db f0                                     	vpand  %ymm0,%ymm6,%ymm6
 1ae:	c5 f5 60 c9                                     	vpunpcklbw %ymm1,%ymm1,%ymm1
 1b2:	c5 ed 60 d2                                     	vpunpcklbw %ymm2,%ymm2,%ymm2
 1b6:	c5 ed d5 c9                                     	vpmullw %ymm1,%ymm2,%ymm1
 1ba:	c5 f5 db c8                                     	vpand  %ymm0,%ymm1,%ymm1
 1be:	c5 f5 67 ce                                     	vpackuswb %ymm6,%ymm1,%ymm1
 1c2:	c5 fe 7f 1c 02                                  	vmovdqu %ymm3,(%rdx,%rax,1)
 1c7:	c5 fe 7f 64 02 20                               	vmovdqu %ymm4,0x20(%rdx,%rax,1)
 1cd:	c5 fe 7f 6c 02 40                               	vmovdqu %ymm5,0x40(%rdx,%rax,1)
 1d3:	c5 fe 7f 4c 02 60                               	vmovdqu %ymm1,0x60(%rdx,%rax,1)
 1d9:	48 83 e8 80                                     	sub    $0xffffffffffffff80,%rax
 1dd:	49 39 c1                                        	cmp    %rax,%r9
 1e0:	0f 85 1a ff ff ff                               	jne    100 <uint8_mul+0x100>
 1e6:	4d 85 d2                                        	test   %r10,%r10
 1e9:	0f 84 b7 fe ff ff                               	je     a6 <uint8_mul+0xa6>
 1ef:	41 83 fa 10                                     	cmp    $0x10,%r10d
 1f3:	0f 82 27 fe ff ff                               	jb     20 <uint8_mul+0x20>
 1f9:	4c 89 c8                                        	mov    %r9,%rax
 1fc:	41 89 ca                                        	mov    %ecx,%r10d
 1ff:	41 83 e2 0f                                     	and    $0xf,%r10d
 203:	4d 89 c1                                        	mov    %r8,%r9
 206:	4d 29 d1                                        	sub    %r10,%r9
 209:	c5 fd 6f 05 00 00 00 00                         	vmovdqa 0x0(%rip),%ymm0        # 211 <uint8_mul+0x211>
 211:	66 2e 0f 1f 84 00 00 00 00 00                   	cs nopw 0x0(%rax,%rax,1)
 21b:	0f 1f 44 00 00                                  	nopl   0x0(%rax,%rax,1)
 220:	c4 e2 7d 30 0c 07                               	vpmovzxbw (%rdi,%rax,1),%ymm1
 226:	c4 e2 7d 30 14 06                               	vpmovzxbw (%rsi,%rax,1),%ymm2
 22c:	c5 ed d5 c9                                     	vpmullw %ymm1,%ymm2,%ymm1
 230:	c5 f5 db c8                                     	vpand  %ymm0,%ymm1,%ymm1
 234:	c4 e3 7d 39 ca 01                               	vextracti128 $0x1,%ymm1,%xmm2
 23a:	c5 f1 67 ca                                     	vpackuswb %xmm2,%xmm1,%xmm1
 23e:	c5 fa 7f 0c 02                                  	vmovdqu %xmm1,(%rdx,%rax,1)
 243:	48 83 c0 10                                     	add    $0x10,%rax
 247:	49 39 c1                                        	cmp    %rax,%r9
 24a:	75 d4                                           	jne    220 <uint8_mul+0x220>
 24c:	4d 85 d2                                        	test   %r10,%r10
 24f:	0f 85 cb fd ff ff                               	jne    20 <uint8_mul+0x20>
 255:	e9 4c fe ff ff                                  	jmp    a6 <uint8_mul+0xa6>
